#!/usr/bin/env python3
"""
Script de conversion du mod√®le scikit-learn vers ONNX.

Ce script convertit le mod√®le ML au format ONNX (Open Neural Network Exchange)
pour am√©liorer la portabilit√©, les performances et √©viter les probl√®mes
de s√©curit√© li√©s √† pickle.

Avantages ONNX:
- Format ouvert et standardis√©
- Performances optimis√©es (runtime ONNX plus rapide)
- Portabilit√© multi-plateforme
- Pas de probl√®mes de s√©curit√© pickle
- Compatible avec de nombreux frameworks

Usage:
    python scripts/convert_to_onnx.py [--input MODEL_PATH] [--output ONNX_PATH]

Exemples:
    # Conversion standard
    python scripts/convert_to_onnx.py

    # Avec chemins personnalis√©s
    python scripts/convert_to_onnx.py --input model/model.pkl \\
        --output model/model.onnx

    # Avec validation
    python scripts/convert_to_onnx.py --validate

Requirements:
    pip install onnx onnxruntime skl2onnx
"""

import argparse
import pickle
import sys
from pathlib import Path
from typing import Optional

import numpy as np
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

# Register LightGBM converter if available
try:
    import lightgbm
    from skl2onnx import update_registered_converter
    from skl2onnx.common.shape_calculator import (
        calculate_linear_classifier_output_shapes
    )
    from onnxmltools.convert.lightgbm.operator_converters.LightGbm import (
        convert_lightgbm
    )

    # Register the LightGBM converter for sklearn-onnx
    update_registered_converter(
        lightgbm.LGBMClassifier,
        'LightGbmLGBMClassifier',
        calculate_linear_classifier_output_shapes,
        convert_lightgbm,
        options={'nocl': [True, False], 'zipmap': [True, False, 'columns']}
    )
    LIGHTGBM_AVAILABLE = True
    print("‚úÖ LightGBM converter enregistr√©")
except ImportError as e:
    LIGHTGBM_AVAILABLE = False
    print(f"‚ö†Ô∏è  Erreur d'import LightGBM converter: {e}")
    print("   Installez avec: uv add onnxmltools")


def load_pickle_model(model_path: str):
    """
    Charge le mod√®le depuis un fichier pickle.

    Args:
        model_path: Chemin vers le fichier .pkl

    Returns:
        Le mod√®le charg√©

    Raises:
        FileNotFoundError: Si le fichier n'existe pas
        Exception: Si le chargement √©choue
    """
    model_file = Path(model_path)
    if not model_file.exists():
        raise FileNotFoundError(f"Mod√®le non trouv√©: {model_path}")

    print(f"üì¶ Chargement du mod√®le depuis {model_path}...")
    with open(model_file, "rb") as f:
        model = pickle.load(f)

    print(f"‚úÖ Mod√®le charg√©: {type(model).__name__}")
    return model


def get_model_info(model) -> dict:
    """
    R√©cup√®re les informations du mod√®le.

    Args:
        model: Le mod√®le scikit-learn

    Returns:
        Dictionnaire avec les informations du mod√®le
    """
    info = {
        "type": type(model).__name__,
        "has_predict": hasattr(model, "predict"),
        "has_predict_proba": hasattr(model, "predict_proba"),
    }

    # R√©cup√©rer le nombre de features
    if hasattr(model, "n_features_in_"):
        info["n_features"] = model.n_features_in_
    elif hasattr(model, "feature_names_in_"):
        info["n_features"] = len(model.feature_names_in_)
    else:
        # Fallback: essayer avec un exemple
        info["n_features"] = None

    # R√©cup√©rer les noms des features si disponibles
    if hasattr(model, "feature_names_in_"):
        info["feature_names"] = list(model.feature_names_in_)
    else:
        info["feature_names"] = None

    # R√©cup√©rer les classes si disponibles
    if hasattr(model, "classes_"):
        info["classes"] = list(model.classes_)
    else:
        info["classes"] = None

    return info


def extract_inference_pipeline(model):
    """
    Extrait la partie du pipeline pertinente pour l'inf√©rence.

    SMOTE et autres techniques de sur-√©chantillonnage ne sont utilis√©es
    qu'√† l'entra√Ænement, pas √† l'inf√©rence. On les retire du pipeline.

    Args:
        model: Le mod√®le (peut √™tre un Pipeline ou un mod√®le simple)

    Returns:
        Le mod√®le d'inf√©rence (sans SMOTE)
    """
    from sklearn.pipeline import Pipeline

    # Si ce n'est pas un Pipeline, retourner tel quel
    if not isinstance(model, Pipeline):
        return model

    # Identifier les √©tapes √† garder (pas SMOTE)
    inference_steps = []
    smote_removed = False

    for name, step in model.steps:
        step_type = type(step).__name__
        # Exclure SMOTE et autres techniques de sur-√©chantillonnage
        if 'SMOTE' in step_type or 'Sampler' in step_type:
            print(f"‚ö†Ô∏è  √âtape '{name}' ({step_type}) retir√©e "
                  "(sur-√©chantillonnage uniquement pour l'entra√Ænement)")
            smote_removed = True
            continue
        inference_steps.append((name, step))

    # Si des √©tapes ont √©t√© retir√©es, cr√©er un nouveau pipeline
    if smote_removed:
        if len(inference_steps) == 1:
            # Si une seule √©tape reste, retourner le mod√®le directement
            print(f"   ‚Üí Pipeline simplifi√© en: {inference_steps[0][1]}")
            return inference_steps[0][1]
        else:
            # Cr√©er un nouveau pipeline
            new_pipeline = Pipeline(inference_steps)
            print(f"   ‚Üí Nouveau pipeline: "
                  f"{' ‚Üí '.join([name for name, _ in inference_steps])}")
            return new_pipeline

    return model


def convert_to_onnx(
    model,
    n_features: int,
    output_path: str,
    target_opset: int = 12
) -> None:
    """
    Convertit le mod√®le scikit-learn en ONNX.

    Args:
        model: Le mod√®le scikit-learn √† convertir
        n_features: Nombre de features d'entr√©e
        output_path: Chemin de sortie pour le fichier ONNX
        target_opset: Version de l'opset ONNX (d√©faut: 12)

    Raises:
        Exception: Si la conversion √©choue
    """
    print("\nüîÑ Conversion du mod√®le en ONNX...")
    print(f"   - Nombre de features: {n_features}")
    print(f"   - Target opset: {target_opset}")

    # Extraire le pipeline d'inf√©rence (sans SMOTE)
    inference_model = extract_inference_pipeline(model)

    # D√©finir le type d'entr√©e (tableau de floats)
    initial_type = [("float_input", FloatTensorType([None, n_features]))]

    # Convertir le mod√®le
    try:
        onnx_model = convert_sklearn(
            inference_model,
            initial_types=initial_type,
            target_opset={'': target_opset, 'ai.onnx.ml': 3},
            options={
                "zipmap": False  # D√©sactiver zipmap pour predict_proba
            }
        )

        # Sauvegarder le mod√®le ONNX
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "wb") as f:
            f.write(onnx_model.SerializeToString())

        print(f"‚úÖ Mod√®le ONNX sauvegard√©: {output_path}")

        # Afficher la taille du fichier
        file_size = output_file.stat().st_size
        print(f"   - Taille: {file_size / 1024:.2f} KB")

    except Exception as e:
        print(f"‚ùå Erreur lors de la conversion: {e}")
        raise


def validate_onnx_model(
    original_model,
    onnx_path: str,
    n_features: int,
    n_samples: int = 10
) -> bool:
    """
    Valide le mod√®le ONNX en comparant avec le mod√®le original.

    Args:
        original_model: Le mod√®le scikit-learn original
        onnx_path: Chemin vers le mod√®le ONNX
        n_features: Nombre de features
        n_samples: Nombre d'√©chantillons de test

    Returns:
        True si la validation r√©ussit, False sinon
    """
    print("\nüß™ Validation du mod√®le ONNX...")

    try:
        import onnxruntime as rt

        # Charger le mod√®le ONNX
        sess = rt.InferenceSession(onnx_path)

        # G√©n√©rer des donn√©es de test al√©atoires
        X_test = np.random.rand(n_samples, n_features).astype(np.float32)

        # Pr√©dictions avec le mod√®le original
        y_sklearn = original_model.predict(X_test)

        # Pr√©dictions avec le mod√®le ONNX
        input_name = sess.get_inputs()[0].name
        output_name = sess.get_outputs()[0].name
        y_onnx = sess.run([output_name], {input_name: X_test})[0]

        # Comparer les r√©sultats
        differences = np.abs(y_sklearn - y_onnx.flatten())
        max_diff = np.max(differences)
        mean_diff = np.mean(differences)

        print(f"   - √âchantillons test√©s: {n_samples}")
        print(f"   - Diff√©rence maximale: {max_diff:.10f}")
        print(f"   - Diff√©rence moyenne: {mean_diff:.10f}")

        # Valider que les diff√©rences sont n√©gligeables
        if max_diff < 1e-5:
            print("‚úÖ Validation r√©ussie: les pr√©dictions sont identiques")
            return True
        else:
            print(f"‚ö†Ô∏è  Diff√©rences d√©tect√©es (max: {max_diff})")
            return False

    except ImportError:
        print("‚ö†Ô∏è  onnxruntime non install√©, validation ignor√©e")
        print("   Installez avec: pip install onnxruntime")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors de la validation: {e}")
        return False


def create_conversion_report(
    input_path: str,
    output_path: str,
    model_info: dict,
    validation_success: Optional[bool] = None
) -> str:
    """
    Cr√©e un rapport de conversion.

    Args:
        input_path: Chemin du mod√®le source
        output_path: Chemin du mod√®le ONNX
        model_info: Informations sur le mod√®le
        validation_success: R√©sultat de la validation

    Returns:
        Le rapport sous forme de string
    """
    report = []
    report.append("=" * 70)
    report.append("RAPPORT DE CONVERSION ONNX")
    report.append("=" * 70)
    report.append("")
    report.append(f"Mod√®le source:  {input_path}")
    report.append(f"Mod√®le ONNX:    {output_path}")
    report.append("")
    report.append("Informations du mod√®le:")
    report.append(f"  - Type:       {model_info['type']}")
    report.append(f"  - Features:   {model_info['n_features']}")

    if model_info["feature_names"]:
        report.append(f"  - Noms:       {len(model_info['feature_names'])} "
                      "features nomm√©es")

    if model_info["classes"]:
        report.append(f"  - Classes:    {model_info['classes']}")

    report.append("")
    report.append("M√©thodes disponibles:")
    predict_icon = '‚úÖ' if model_info['has_predict'] else '‚ùå'
    report.append(f"  - predict:        {predict_icon}")
    predict_proba_icon = '‚úÖ' if model_info['has_predict_proba'] else '‚ùå'
    report.append(f"  - predict_proba:  {predict_proba_icon}")

    if validation_success is not None:
        report.append("")
        report.append("Validation:")
        validation_icon = '‚úÖ R√©ussie' if validation_success else '‚ùå √âchec'
        report.append(f"  - Statut:     {validation_icon}")

    report.append("")
    report.append("Prochaines √©tapes:")
    report.append("  1. V√©rifier que le mod√®le ONNX fonctionne correctement")
    report.append("  2. Mettre √† jour l'API pour utiliser onnxruntime")
    report.append("  3. Mettre √† jour les tests unitaires")
    report.append("  4. D√©ployer la nouvelle version")
    report.append("")
    report.append("Documentation:")
    report.append("  - ONNX Runtime: https://onnxruntime.ai/")
    report.append("  - skl2onnx: https://onnx.ai/sklearn-onnx/")
    report.append("=" * 70)

    return "\n".join(report)


def main():
    """Point d'entr√©e principal du script."""
    parser = argparse.ArgumentParser(
        description="Convertit un mod√®le scikit-learn en ONNX",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    parser.add_argument(
        "--input",
        "-i",
        default="model/model.pkl",
        help="Chemin du mod√®le pickle (d√©faut: model/model.pkl)"
    )
    parser.add_argument(
        "--output",
        "-o",
        default="model/model.onnx",
        help="Chemin de sortie ONNX (d√©faut: model/model.onnx)"
    )
    parser.add_argument(
        "--validate",
        action="store_true",
        help="Valider le mod√®le ONNX apr√®s conversion"
    )
    parser.add_argument(
        "--n-features",
        type=int,
        default=28,
        help="Nombre de features (d√©faut: 28)"
    )
    parser.add_argument(
        "--opset",
        type=int,
        default=12,
        help="Version de l'opset ONNX (d√©faut: 12)"
    )

    args = parser.parse_args()

    try:
        # Charger le mod√®le
        model = load_pickle_model(args.input)

        # R√©cup√©rer les informations du mod√®le
        model_info = get_model_info(model)

        # Utiliser le nombre de features du mod√®le si disponible
        n_features = model_info["n_features"] or args.n_features
        print(f"   - Features: {n_features}")

        # Convertir en ONNX
        convert_to_onnx(
            model,
            n_features=n_features,
            output_path=args.output,
            target_opset=args.opset
        )

        # Valider si demand√©
        validation_success = None
        if args.validate:
            validation_success = validate_onnx_model(
                model,
                args.output,
                n_features
            )

        # Cr√©er et afficher le rapport
        report = create_conversion_report(
            args.input,
            args.output,
            model_info,
            validation_success
        )
        print("\n" + report)

        # Code de sortie
        if validation_success is False:
            sys.exit(1)
        else:
            sys.exit(0)

    except Exception as e:
        print(f"\n‚ùå Erreur: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
